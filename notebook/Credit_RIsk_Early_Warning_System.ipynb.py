# -*- coding: utf-8 -*-
"""Credit RIsk Early Warning System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N5EFSzH-fCfk6HOlyErimoAqQnI7SK65
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 0. Setup

## 0.1 Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as ss
from scipy.stats import pointbiserialr
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, average_precision_score
from sklearn.metrics import precision_recall_curve, auc, roc_curve
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold, cross_val_score

import joblib

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

"""# 1. Data Loading

## 1.1 Importing Data
"""

filepath = "/content/drive/MyDrive/self learning/Self Project/Credit Risk EWS/SBAnational.csv"
df = pd.read_csv(filepath)
df

print("Value counts for 'MIS_Status' column:")
display(df['MIS_Status'].value_counts())

"""* 'CHGOFF' stands for 'Charged Off', which means the loan has defaulted.
 * 'P I F' means 'Paid in Full'.

## 1.2 Check Data Types
"""

df.dtypes

"""# 2. Data Preprocessing

## 2.1 Handling Data Types
"""

# Converting date columns to datetime
date_cols = ['ApprovalDate', 'ChgOffDate', 'DisbursementDate']
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce')

# Converting currency columns and removing $ sign
currency_cols = ['DisbursementGross', 'BalanceGross', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv']
for col in currency_cols:
  df[col] = df[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False).astype(float)

print('New data types:')
df.dtypes

"""## 2.2. Handling Missing Values"""

missing_df = df.isnull().sum()
missing_df

missing_percentage = missing_df / len(df) * 100
missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)

plt.figure(figsize=(12, 6))
missing_percentage.plot(kind='bar')
plt.title('Percentage of Missing Values per Column')
plt.xlabel('Columns')
plt.ylabel('Percentage Missing (%)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Handling missing values

def handle_missing_values(df):
  df = df.copy()
  df = df.dropna(subset=["MIS_Status"])
  df["RevLineCr"] = df["RevLineCr"].fillna("N")
  df["LowDoc_missing_flag"] = df["LowDoc"].isna().astype(int)
  df["LowDoc"] = df["LowDoc"].fillna("N")
  df = df.dropna(subset=["DisbursementDate"])
  df["Bank"] = df["Bank"].fillna("Unknown")
  df["BankState"] = df["BankState"].fillna("Unknown")
  df["NewExist_missing_flag"] = df["NewExist"].isna().astype(int)
  df["NewExist"] = df["NewExist"].fillna(1)

  for col in ["City", "State", "Name"]:
      df[col] = df[col].fillna("Unknown")

  return df

# Apply the function
df = handle_missing_values(df)

df["NAICS"] = df["NAICS"].astype(str)
df["NAICS"] = df["NAICS"].replace("0", pd.NA)
df["NAICS"] = df["NAICS"].fillna("Unknown")

df["NAICS_2"] = df["NAICS"].astype(str).str[:2]

df.isnull().sum()

# Verify missing values after handling
missing_after = df.isnull().sum()
missing_after_filtered = missing_after[missing_after > 0].sort_values(ascending=False)
print("Remaining missing values:")
display(missing_after_filtered)

print("Value counts for 'MIS_Status' column:")
display(df['MIS_Status'].value_counts())

"""# 3. Exploratory Data Analysis

## 3.1 Univariate Analysis
"""

# Basic Overview
df.shape, df.dtypes

# Summary Statistics
df.describe()

# Summary Statistics
df.select_dtypes(include="object").describe()

# Distribution of Loan Amount
df["DisbursementGross"].describe()

# Distribution Approval Fiscal Year
df['ApprovalFY_numeric'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')
df['ApprovalFY_numeric'].value_counts().sort_index()

"""## 3.2 Bivariate Analysis (default vs features)

### Default Rate Overall
"""

# Create the 'default' column based on 'MIS_Status'
df['default'] = df['MIS_Status'].apply(lambda x: 1 if x == 'CHGOFF' else 0)
print(" 'default' column created successfully.")
display(df[['MIS_Status', 'default']].head(10))

"""### Default Status Count"""

# Show default status
df["default"].value_counts()

"""### Default Rate by NAICS 2-digit"""

# Default Rate by NAICS 2-digit
df.groupby("NAICS_2")["default"].mean().sort_values(ascending=False)

"""### Default Rate by Approval Fiscal Year Explanation"""

# Default Rate by Approval Fiscal Year
# Ensure 'ApprovalFY_numeric' is used for grouping and sorting to avoid type errors
default_rate_by_fy = df.groupby('ApprovalFY_numeric')['default'].mean().sort_index()
display(default_rate_by_fy)

"""### Default Rate by Loan Size Bucket"""

# Create five quantiles for 'DisbursementGross' and label them
df['LoanSizeBucket'] = pd.qcut(df['DisbursementGross'],
                                 q=5,
                                 labels=['Small', 'Medium-Small', 'Medium', 'Medium-Large', 'Large'],
                                 duplicates='drop')

# Calculate the default rate for each loan size bucket
default_rate_by_loan_size = df.groupby('LoanSizeBucket')['default'].mean()

print("Default Rate by Loan Size Bucket:")
display(default_rate_by_loan_size)

"""### Default Rate by Business Age (NewExist)"""

print("Default Rate by NewExist:")
display(df.groupby('NewExist')['default'].mean())

"""### Default Rate by Documentation Quality"""

print("Default Rate by LowDoc:")
display(df.groupby('LowDoc')['default'].mean())

"""### Default Rate by Urban vs Rural"""

print("Default Rate by UrbanRural:")
display(df.groupby('UrbanRural')['default'].mean())

"""### Interaction: NAICS + Loan Bucket"""

print("Default Rate by NAICS_2 and Loan Size Bucket:")
display(df.groupby(['NAICS_2', 'LoanSizeBucket'])['default'].mean().unstack())

"""### PD Trend Chart"""

plt.figure(figsize=(12, 6))
default_rate_by_fy.plot(kind='line')
plt.title('Probability of Default (PD) Over Time by Approval Fiscal Year')
plt.xlabel('Approval Fiscal Year')
plt.ylabel('Default Rate')
plt.grid(True)
plt.show()

"""## Multivariate Risk Summary Table

Below is a summary of default rates across key risk factors identified through the Exploratory Data Analysis.

| Risk Factor               | Category       | Default Rate |
|:--------------------------|:---------------|:-------------|
| **Loan Size Bucket**      | Small          | 0.2556       |
|                           | Medium-Small   | 0.2116       |
|                           | Medium         | 0.1803       |
|                           | Medium-Large   | 0.1341       |
|                           | Large          | 0.0917       |
| **Business Age (NewExist)** | 0.0 (Unknown)  | 0.0613       |
|                           | 1.0 (Existing) | 0.1712       |
|                           | 2.0 (New)      | 0.1876       |
| **Documentation Quality (LowDoc)** | A       | 0.3232       |
|                           | C              | 0.0991       |
|                           | N              | 0.1878       |
|                           | R              | 0.3378       |
|                           | S              | 0.4508       |
|                           | Y              | 0.0899       |
| **Urban/Rural Classification** | 0 (Undefined)  | 0.0707       |
|                           | 1 (Urban)      | 0.2453       |
|                           | 2 (Rural)      | 0.1879       |

### Key Observations:
*   **Loan Size**: Smaller loans (`Small` bucket) exhibit the highest default rates, with default rate decreasing as loan size increases.
*   **Business Age**: New businesses (`NewExist` = 2.0) generally have a higher default rate compared to existing businesses (`NewExist` = 1.0). The category 0.0 (Unknown) shows the lowest default rate.
*   **Documentation Quality**: Loans with `LowDoc` categories 'S', 'R', and 'A' show significantly higher default rates. 'N' (Not LowDoc) and 'Y' (LowDoc) have lower default rates. It's important to note the mixed categories ('0', '1', 'C') for `LowDoc` and their respective default rates which might need further investigation.
*   **Urban/Rural Classification**: Loans in 'Urban' areas (`UrbanRural` = 1) have a higher default rate than those in 'Rural' areas (`UrbanRural` = 2), while `UrbanRural` = 0 (Undefined) has the lowest.
*   **NAICS_2 and Loan Size**: The interaction table shows varying default rates across industries and loan sizes, indicating that certain industries combined with specific loan size buckets pose higher risks (e.g., NAICS_2 53, 52, 51 often show high default rates for `Small` and `Medium-Small` loans).
*   **Probability of Default Over Time**: The trend chart reveals the evolution of default rates over the years, which can highlight periods of increased risk (e.g., economic downturns).

## Summary:

### Q&A
The comprehensive Exploratory Data Analysis identified the following key risk factors for loan defaults:
*   **Loan Size**: Smaller loans ('Small' bucket) have the highest default rate (0.2556), with rates decreasing for larger loans, down to 0.0917 for 'Large' loans.
*   **Business Age ('NewExist')**: 'New' businesses (2.0) show the highest default rate (0.1876), followed by 'Existing' businesses (1.0) at 0.1712.
*   **Documentation Quality ('LowDoc')**: Categories 'S', 'R', and 'A' have significantly higher default rates (0.4508, 0.3378, and 0.3232 respectively).
*   **Urban/Rural Classification ('UrbanRural')**: Loans in 'Urban' areas (1) have a higher default rate (0.2453) than those in 'Rural' areas (2) at 0.1879.
*   **NAICS 2-digit codes and loan size buckets**: Certain industries (e.g., NAICS\_2 52 and 53) combined with 'Small' and 'Medium-Small' loan sizes show elevated default rates.
*   **Probability of Default (PD) over time**: The trend chart visualizes the evolution of default rates over approval fiscal years, indicating periods of increased risk.

### Data Analysis Key Findings
*   There is a clear inverse relationship between loan size and default rate; smaller loans (e.g., 'Small' bucket at 0.2556) have significantly higher default rates compared to larger loans (e.g., 'Large' bucket at 0.0917).
*   New businesses ('NewExist' = 2.0) exhibit a higher default rate (0.1876) than existing businesses ('NewExist' = 1.0) at 0.1712.
*   Specific documentation quality categories, particularly 'S', 'R', and 'A' in 'LowDoc', are associated with substantially higher default rates (0.4508, 0.3378, and 0.3232, respectively).
*   Loans in urban areas ('UrbanRural' = 1) show a higher default rate (0.2453) compared to rural areas ('UrbanRural' = 2) at 0.1879.
*   Certain industry sectors (NAICS\_2 codes like 52 for Finance and Insurance, and 53 for Real Estate and Rental) combined with 'Small' and 'Medium-Small' loan sizes consistently present higher default risks.

### Insights or Next Steps
*   **Targeted Risk Mitigation**: Lenders should consider implementing stricter underwriting criteria or offering additional support for small loans, new businesses, loans with 'S', 'R', or 'A' 'LowDoc' categories, and loans in urban areas, given their elevated default rates.
*   **Further Investigation of LowDoc Categories**: The mixed categories within 'LowDoc' suggest a need for clearer definitions or additional data collection on documentation quality to better understand and predict default risk.

## 3.3 Correlation Analysis

### 3.3.1 Pearson Correlation (Heatmap)
"""

numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns.tolist()

plt.figure(figsize=(12, 8))
sns.heatmap(df[numeric_cols].corr(), annot=False, cmap="coolwarm")
plt.title("Pearson Correlation Heatmap (Numeric Features)")
plt.show()

"""### 3.3.2 Point-Biserial Correlation (Numeric vs Default)"""

numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns.tolist()
numeric_cols = [col for col in numeric_cols if col not in ["default"]]  # exclude target

pb_corr = {}

for col in numeric_cols:
    r, p = pointbiserialr(df[col], df["default"])
    pb_corr[col] = {"correlation": r, "p_value": p}

pd.DataFrame(pb_corr).T.sort_values("correlation", ascending=False)

"""### 3.3.3 Cramer’s V (Categorical vs Default)"""

def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    r,k = confusion_matrix.shape
    return np.sqrt(chi2/n / (min(k-1, r-1)))

cat_cols = ["NAICS_2", "LowDoc", "RevLineCr", "UrbanRural"]

cramer_results = {col: cramers_v(df[col], df["default"]) for col in cat_cols}
pd.DataFrame.from_dict(cramer_results, orient="index", columns=["CramersV"])

"""## 3.4. Visualization

### 3.4.1 Default Rate by NAICS 2-digit
"""

df.groupby("NAICS_2")["default"].mean().sort_values(ascending=False).plot(kind="bar", figsize=(10,5))
plt.title("Default Rate by NAICS 2-digit")
plt.show()

"""### 3.4.2 Default Rate by Loan Amount Bucket"""

df["loan_bucket"] = pd.qcut(df["DisbursementGross"], q=10)
df.groupby("loan_bucket")["default"].mean().plot(kind="bar", figsize=(10,5))
plt.title("Default Rate by Loan Amount Bucket")
plt.show()

"""### 3.4.3 Default Rate by Year"""

df.groupby("ApprovalFY")["default"].mean().plot(kind="line", marker="o", figsize=(10,5))
plt.title("Default Rate by Approval Fiscal Year")
plt.show()

"""### 3.4.4 Default Rate by LowDoc Flag"""

df.groupby("LowDoc")["default"].mean().plot(kind="bar", figsize=(6,4))
plt.title("Default Rate by Low Documentation")
plt.show()

"""# 4. Feature Engineering

### 4.1 Create Target Variables (PD & LGD)

PD (Probability of Default) → binary classification

LGD (Loss Given Default) → continuous regression (severity of loss)

### 4.1.1 PD Target (default)

# Clean MIS_status (remove spaces) for verification
df["MIS_Status_clean"] = df["MIS_Status"].str.replace(" ", "").str.upper()

# Verify PD target variable matches
print("Verification: default column already created correctly")
print(f"Default rate: {df['default'].mean():.4f}")
"""

# Clean MIS_status (remove spaces)
df["MIS_Status_clean"] = df["MIS_Status"].str.replace(" ", "").str.upper()

# Verify PD target variable
print("Verification: 'default' column already exists")
print(f"Default rate: {df['default'].mean():.4f}")
print(f"Match with MIS_Status: {(df['default'] == (df['MIS_Status'] == 'CHGOFF').astype(int)).all()}")

"""### 4.1.2 LGD Target (LGD)"""

# Safe LGD calculation with handling for edge cases
df["LGD"] = np.where(
    df["DisbursementGross"] > 0,
    df["ChgOffPrinGr"] / df["DisbursementGross"],
    0
)

# Cap LGD at 1 (100%) as it shouldn't exceed disbursed amount
df["LGD"] = df["LGD"].clip(upper=1)

# For non-default loans, LGD must be 0
df.loc[df["default"] == 0, "LGD"] = 0

# Handle any remaining NaN or inf values
df["LGD"] = df["LGD"].fillna(0)

print(f"LGD Statistics for defaulted loans:")
print(df[df["default"] == 1]["LGD"].describe())

"""## 4.2 Core Feature Engineering (Risk Drivers)

### 4.2.1 Log Loan Amount
"""

df["log_loan_amt"] = np.log(df['DisbursementGross'] + 1)

"""### 4.2.2 NAICS 2-digit Industry Code"""

# Ensuring consistency
df["NAICS"] = df["NAICS"].astype(str).str[:6]  # ensure string

# Verify NAICS_2 already created
print(f"NAICS_2 already created. Unique values: {df['NAICS_2'].nunique()}")
print("\nTop 10 NAICS_2 categories:")
print(df['NAICS_2'].value_counts().head(10))

"""### 4.2.3 New Business Flag"""

df['new_business'] = (df["NewExist"] == 2).astype(int)

"""### 4.2.4 Low Documentation Flag"""

df["low_doc"] = (df["LowDoc"] == "Y").astype(int)

"""### 4.2.5 Urban vs Rural"""

df["urban_flag"] = (df["UrbanRural"] == 1).astype(int)
df["urban_flag"].value_counts()

"""* 1 = Urban
* 0 = Rural

### 4.2.6 Employment Size Bucket
"""

# Business Size could be a strong predictor of PD
df["emp_bucket"] = pd.qcut(df["NoEmp"], q=10, labels=False, duplicates="drop")

"""### 4.2.7 Loan Term Bucket"""

df["term_bucket"] = pd.qcut(df["Term"], q=10, labels=False, duplicates="drop")

"""### 4.2.8 Fiscal Year as Numeric (macro environment)"""

df["ApprovalFY"] = pd.to_numeric(df["ApprovalFY"], errors='coerce').fillna(0).astype(int)

"""## 4.3 Final Feature for PD Model"""

pd_features = [
    "Term",
    "NoEmp",
    "log_loan_amt",
    "new_business",
    "low_doc",
    "urban_flag",
    "ApprovalFY",
    "NAICS_2"
]

X_pd = df[pd_features]
y_pd = df["default"]

"""## 4.4 Final Feature Set for LGD Model"""

lgd_features = [
    "Term",
    "NoEmp",
    "log_loan_amt",
    "new_business",
    "low_doc",
    "urban_flag",
    "ApprovalFY",
    "NAICS_2"
]

X_lgd = df[lgd_features]
y_lgd = df["LGD"]

"""# 5. Model Development

## Model Development Steps:

1. **Train-Test Split**: Split data into training and testing sets
2. **Model Selection**: Choose appropriate algorithms (e.g., Logistic Regression, Random Forest, XGBoost)
3. **Model Training**: Train PD and LGD models separately
4. **Model Evaluation**: Evaluate using appropriate metrics (AUC-ROC, Precision-Recall for PD; RMSE, MAE for LGD)
5. **Model Calibration**: Calibrate probability outputs
6. **Model Validation**: Cross-validation and backtesting
7. **Model Deployment**: Save models and create prediction pipeline

## 5.1 Train-Test Split (with Stratification for PD model only)
"""

# PD = stratified (imbalance dataset)
X_train_pd, X_test_pd, y_train_pd, y_test_pd = train_test_split(X_pd,
                                                                y_pd,
                                                                test_size=0.2,
                                                                stratify=y_pd,
                                                                random_state=42)
# LGD = standard split (continous target)
X_train_lgd, X_test_lgd, y_train_lgd, y_test_lgd = train_test_split(X_lgd,
                                                                  y_lgd,
                                                                  test_size=0.2,
                                                                  random_state=42)

"""## 5.2 Preprocessing Pipeline"""

# Get all unique categories for categorical features from the full dataset for consistent encoding
all_naics_2_categories = sorted(df['NAICS_2'].unique().tolist())
all_approval_fy_categories = sorted(df['ApprovalFY'].unique().tolist())

numeric_features = ["Term", "NoEmp", "log_loan_amt"]
binary_features = ["new_business", "low_doc", "urban_flag"]
categorical_features = ["NAICS_2", "ApprovalFY"]

# Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("bin", "passthrough", binary_features),
        ("cat", OneHotEncoder(handle_unknown="ignore", categories=[all_naics_2_categories, all_approval_fy_categories]), categorical_features)
    ]
)

"""## 5.3 Model Training

## 5.3.1 Logistic Regression (PD Baseline)
"""

logreg_pd = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", LogisticRegression(max_iter=1000))
])

logreg_pd.fit(X_train_pd, y_train_pd)

"""### 5.3.2 Random Forest (PD Benchmark)"""

rf_pd = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", RandomForestClassifier(
        n_estimators=300,
        max_depth=12,
        class_weight="balanced",
        random_state=42
    ))
])

rf_pd.fit(X_train_pd, y_train_pd)

"""### 5.3.3 XGBoost Classifier (PD Final Model)"""

xgb_pd = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", XGBClassifier(
        objective="binary:logistic",
        eval_metric="logloss",
        learning_rate=0.05,
        n_estimators=400,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.7,
        random_state=42
    ))
])

xgb_pd.fit(X_train_pd, y_train_pd)

"""### 5.3.4 Linear Regression (LGD Baseline)"""

lr_lgd = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", LinearRegression())
])

lr_lgd.fit(X_train_lgd, y_train_lgd)

"""### 5.3.5 XGBoost Regressor (LGD Final Model)"""

xgb_lgd = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", XGBRegressor(
        objective="reg:squarederror",
        n_estimators=400,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.7,
        random_state=42
    ))
])

xgb_lgd.fit(X_train_lgd, y_train_lgd)

"""## 5.4 Model Evaluation

### 5.4.1 PD Evaluation Function
"""

def evaluate_pd_model(model, X_test, y_test):
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_pred_proba >= 0.5).astype(int)

    roc = roc_auc_score(y_test, y_pred_proba)
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
    pr_auc = auc(recall, precision)
    f1 = f1_score(y_test, y_pred)

    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    ks = max(tpr - fpr)

    print("ROC-AUC:", roc)
    print("PR-AUC:", pr_auc)
    print("F1 Score:", f1)
    print("KS Statistic:", ks)
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    return roc, pr_auc, ks

"""### 5.4.2 LGD Evaluation Function"""

def evaluate_lgd_model(model, X_test, y_test):
    preds = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)

    print("RMSE:", rmse)
    print("MAE:", mae)
    print("R²:", r2)

    return rmse, mae, r2

"""### 5.4.3 Run Evaluations"""

print("=== Logistic Regression PD ===")
evaluate_pd_model(logreg_pd, X_test_pd, y_test_pd)

print("\n=== Random Forest PD ===")
evaluate_pd_model(rf_pd, X_test_pd, y_test_pd)

print("\n=== XGBoost PD (FINAL) ===")
evaluate_pd_model(xgb_pd, X_test_pd, y_test_pd)

"""## Final Summary: Probability of Default (PD) Modeling

The PD modeling results show a clear performance progression across the three algorithms: Logistic Regression, Random Forest, and XGBoost. Each model provides valuable insights into borrower credit risk, with measurable improvements in discrimination and recall of default cases.

---

### **1. Logistic Regression (Baseline)**
Logistic Regression establishes a strong and interpretable baseline:
- **ROC-AUC: 0.863**
- **PR-AUC: 0.635**
- **KS Statistic: 0.586**
- **F1 Score: 0.51**

These results indicate that the engineered features capture meaningful patterns related to borrower behavior and loan performance. The KS value and PR-AUC show that the model is already effective in distinguishing defaults within an imbalanced dataset.

---

### **2. Random Forest**
Random Forest provides noticeable improvement, particularly in non-linear separability:
- **ROC-AUC: 0.918**
- **PR-AUC: 0.734**
- **KS Statistic: 0.696**
- **F1 Score: 0.66**

The gain in metrics suggests that tree-based models can capture interactions and complex decision boundaries that are not accessible to linear models. This results in stronger default detection with a more flexible modeling structure.

---

### **3. XGBoost (Final Model)**
XGBoost delivers the strongest performance in this project:
- **ROC-AUC: 0.968**
- **PR-AUC: 0.879**
- **KS Statistic: 0.826**
- **F1 Score: 0.82**

The model effectively balances precision and recall, handles class imbalance well, and achieves high discriminatory power. The improvement across all metrics reflects XGBoost’s ability to model non-linear and high-dimensional structure in the transformed dataset.

---

### **Overall Assessment**
Based on the evaluation metrics, **XGBoost is selected as the final PD model**.  
Its strong classification performance, especially on PR-AUC and KS, makes it suitable for use in a credit risk Early Warning System where early detection of potential defaults is essential for risk mitigation and portfolio monitoring.
"""

print("=== Linear Regression LGD ===")
evaluate_lgd_model(lr_lgd, X_test_lgd, y_test_lgd)

print("\n=== XGBoost LGD (FINAL) ===")
evaluate_lgd_model(xgb_lgd, X_test_lgd, y_test_lgd)

"""## Final Summary: Loss Given Default (LGD) Modeling

LGD modeling is generally more challenging than PD modeling due to the nature of the target variable, which tends to be noisy, zero-inflated, and influenced by factors that are not captured in the dataset. In this project, both Linear Regression and XGBoost were developed to estimate LGD values.

---

### **1. Linear Regression (Baseline)**
- **RMSE: 0.238**
- **MAE: 0.106**
- **R²: 0.158**

The Linear Regression model provides a reasonable starting point.  
The R² value is modest, which is expected given the variability of LGD and the limited availability of recovery-related information. Error-based metrics (RMSE and MAE) give a clearer picture of model performance and indicate that the baseline model has acceptable predictive accuracy for a first iteration.

---

### **2. XGBoost (Final Model)**
- **RMSE: 0.166**
- **MAE: 0.082**
- **R²: 0.589**

XGBoost shows a substantial improvement in prediction accuracy.  
The lower RMSE and MAE values suggest that the model captures non-linear patterns more effectively. The higher R² indicates better explanatory strength without overfitting, supported by consistent performance on the test set.

---

### **Overall Assessment**
The LGD results highlight the importance of using models that can accommodate complex relationships and distributional challenges. **XGBoost is selected as the final LGD model** based on its significantly lower prediction error and improved fit. While LGD remains inherently difficult to model, the final model provides a stable and practical estimate suitable for integration into a broader credit risk framework alongside the PD model.

## 5.5 Probability Calibration (PD Only)
"""

calibrated_xgb = CalibratedClassifierCV(
    estimator=xgb_pd,
    method="sigmoid",
    cv=3
)

calibrated_xgb.fit(X_train_pd, y_train_pd)

print("=== Calibrated XGBoost PD ===")
evaluate_pd_model(calibrated_xgb, X_test_pd, y_test_pd)

"""## 5.6 Cross-Validation (PD)"""

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_auc = cross_val_score(
    xgb_pd,
    X_pd,
    y_pd,
    cv=skf,
    scoring="roc_auc",
    n_jobs=-1
)

print(cv_auc)
print("Mean:", cv_auc.mean())

"""## 5.7 Train vs Test Performance Comparison

PD
"""

# Train vs Test comparison for PD

def evaluate_pd_split(model, X_train, y_train, X_test, y_test):

    # TRAIN
    train_proba = model.predict_proba(X_train)[:, 1]
    train_pred  = (train_proba >= 0.5).astype(int)

    # TEST
    test_proba = model.predict_proba(X_test)[:, 1]
    test_pred  = (test_proba >= 0.5).astype(int)

    print("=== TRAIN PERFORMANCE ===")
    print("ROC-AUC:", roc_auc_score(y_train, train_proba))
    print("PR-AUC :", average_precision_score(y_train, train_proba))
    print("F1     :", f1_score(y_train, train_pred))

    print("\n=== TEST PERFORMANCE ===")
    print("ROC-AUC:", roc_auc_score(y_test, test_proba))
    print("PR-AUC :", average_precision_score(y_test, test_proba))
    print("F1     :", f1_score(y_test, test_pred))

    print("\nAUC Gap:", roc_auc_score(y_train, train_proba) - roc_auc_score(y_test, test_proba))
    print("PR Gap :", average_precision_score(y_train, train_proba) - average_precision_score(y_test, test_proba))

print("\n=== XGBoost PD — Train vs Test Check ===")
evaluate_pd_split(xgb_pd, X_train_pd, y_train_pd, X_test_pd, y_test_pd)

"""No overfit.

LGD
"""

# Train vs Test comparison for LGD
def evaluate_lgd_split(model, X_train, y_train, X_test, y_test):

    # TRAIN
    train_pred = model.predict(X_train)
    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
    train_mae  = mean_absolute_error(y_train, train_pred)
    train_r2   = r2_score(y_train, train_pred)

    # TEST
    test_pred  = model.predict(X_test)
    test_rmse  = np.sqrt(mean_squared_error(y_test, test_pred))
    test_mae   = mean_absolute_error(y_test, test_pred)
    test_r2    = r2_score(y_test, test_pred)

    print("=== TRAIN PERFORMANCE ===")
    print("RMSE:", train_rmse)
    print("MAE :", train_mae)
    print("R²  :", train_r2)

    print("\n=== TEST PERFORMANCE ===")
    print("RMSE:", test_rmse)
    print("MAE :", test_mae)
    print("R²  :", test_r2)

    print("\nRMSE Gap:", train_rmse - test_rmse)
    print("MAE Gap :", train_mae - test_mae)
    print("R² Gap  :", train_r2 - test_r2)

print("\n=== XGBoost LGD — Train vs Test Check ===")
evaluate_lgd_split(xgb_lgd, X_train_lgd, y_train_lgd, X_test_lgd, y_test_lgd)

"""No overfit.

## 5.8 Hyperparameter Tuning
"""

X_train_base, X_val, y_train_base, y_val = train_test_split(
    X_train_pd,
    y_train_pd,
    test_size=0.2,
    stratify=y_train_pd,
    random_state=42
)

param_grid = {
    "model__n_estimators": [200, 300, 400, 600],
    "model__max_depth": [3, 4, 5, 6, 7],
    "model__learning_rate": [0.01, 0.05, 0.1],
    "model__subsample": [0.6, 0.7, 0.8, 1.0],
    "model__colsample_bytree": [0.6, 0.7, 0.8, 1.0],
    "model__gamma": [0, 0.1, 0.2],
    "model__min_child_weight": [1, 3, 5],
    "model__scale_pos_weight": [1, 5, 10]
}

xgb_base = XGBClassifier(
    objective="binary:logistic",
    eval_metric="logloss",
    tree_method="hist"
)

# Create a pipeline that includes preprocessing and the XGBoost model
xgb_pipeline_for_tuning = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", xgb_base)
])

random_search = RandomizedSearchCV(
    estimator=xgb_pipeline_for_tuning, # Use the pipeline here
    param_distributions=param_grid,
    n_iter=20,
    scoring="roc_auc",
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train_pd, y_train_pd)

best_xgb_pd = random_search.best_estimator_
print("Best PD Params:", random_search.best_params_)

print("\n=== Tuned XGBoost PD — Test Performance ===")
evaluate_pd_model(best_xgb_pd, X_test_pd, y_test_pd)

import joblib

# PD final tuned model
joblib.dump(best_xgb_pd, "PD_model_tuned_pipeline.pkl")

# LGD model (tuned or untuned)
joblib.dump(xgb_lgd, "LGD_model_pipeline.pkl")

# PD calibrated version (optional)
joblib.dump(calibrated_xgb, "PD_model_calibrated_pipeline.pkl")

print("All models saved successfully!")

"""# 6. Macro Stress Testing

## 6.1 Download Macro Data
"""

import yfinance as yf

import yfinance as yf
import pandas as pd
import numpy as np

vix_df = yf.download("^VIX", start="2019-01-01", end="2025-01-01")
tnx_df = yf.download("^TNX", start="2019-01-01", end="2025-01-01")
sp500_df = yf.download("^GSPC", start="2019-01-01", end="2025-01-01")


vix_close = vix_df['Close'] if 'Close' in vix_df.columns and not vix_df.empty else pd.Series(dtype=float)
tnx_close = tnx_df['Close'] if 'Close' in tnx_df.columns and not tnx_df.empty else pd.Series(dtype=float)
sp500_close = sp500_df['Close'] if 'Close' in sp500_df.columns and not sp500_df.empty else pd.Series(dtype=float)

# Calculate percentage change for SP500
sp500_pct_change = sp500_close.pct_change() if len(sp500_close) > 1 else pd.Series(dtype=float)

macro = pd.concat({
    "VIX": vix_close,
    "TNX": tnx_close,
    "SP500": sp500_pct_change
}, axis=1).dropna(how='all')
macro = macro.dropna()

print("Macro dataset preview:")
display(macro.tail())

"""## 6.2 Define Stress Logic"""

def classify_stress_level(vix_value):
  if vix_value < 20:
    return "GREEN"
  elif vix_value < 30:
    return "YELLOW"
  else:
    return "RED"

def stress_multiplier(stress):
  if stress == "GREEN":
    return 1.0
  elif stress == "YELLOW":
    return 1.3
  else:
    return 1.6

"""## 6.3 Current Stress Level"""

latest_vix_scalar = macro["VIX"].iloc[-1].item()
current_stress = classify_stress_level(latest_vix_scalar)
mult = stress_multiplier(current_stress)

print("\n=== Stress Test Settings ===")
print("Latest VIX:", latest_vix_scalar)
print("Stress Level:", current_stress)
print("Stress Multiplier:", mult)

"""## 6.4 Apply Stress to PD"""

df["PD_baseline"] = best_xgb_pd.predict_proba(X_pd)[:, 1]
df["PD_stressed"] = (df["PD_baseline"] * mult).clip(0, 1)

print("\nPD Stress Test Preview")
display(df[["PD_baseline", "PD_stressed"]].head())

"""## 6.5 Compute Expected Loss Stressed"""

df["EL_baseline"] = df["PD_baseline"] * df["LGD"] * df["DisbursementGross"]
df["EL_stressed"] = df["PD_stressed"] * df["LGD"] * df["DisbursementGross"]

print("\nEL Stress Test Preview")
display(df[["EL_baseline", "EL_stressed"]].head())

"""## 6.6 Portfolio-Level Impact"""

portfolio_EL_base = df["EL_baseline"].sum()
portfolio_EL_stress = df["EL_stressed"].sum()

print("\n=== Portfolio Expected Loss ===")
print("Baseline EL:", round(portfolio_EL_base, 2))
print("Stressed EL:", round(portfolio_EL_stress, 2))
print("Increase (%):", round((portfolio_EL_stress - portfolio_EL_base) / portfolio_EL_base * 100, 2))

"""## 6.7 Summary Table"""

stress_summary = pd.DataFrame({
    "Metric": ["Baseline_EL", "Stressed_EL", "Stress Level", "Multiplier"],
    "Value": [
        portfolio_EL_base,
        portfolio_EL_stress,
        current_stress,
        mult
    ]
})

print("\n=== Stress Testing Summary ===")
display(stress_summary)

